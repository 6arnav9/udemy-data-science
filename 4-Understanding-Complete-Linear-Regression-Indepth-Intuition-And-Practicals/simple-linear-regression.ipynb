{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745b7340",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Simple Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Notes\n",
    "\n",
    "![Simple Linear Regression 1](img/slr-1.png)\n",
    "![Simple Linear Regression 2](img/slr-2.png)\n",
    "![Simple Linear Regression 3](img/slr-3.png)\n",
    "![Simple Linear Regression 4](img/slr-4.png)\n",
    "![Simple Linear Regression 5](img/slr-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f227a54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is Simple Linear Regression?\n",
    "\n",
    "Simple Linear Regression is a supervised machine learning algorithm used to predict a continuous dependent variable \\( y \\) based on one independent variable \\( x \\).\n",
    "\n",
    "The relationship is modeled as a straight line:\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "- \\( \\theta_0 \\): Intercept (where the line crosses the y-axis)\n",
    "- \\( \\theta_1 \\): Slope (how much \\( y \\) changes for a unit change in \\( x \\))\n",
    "\n",
    "---\n",
    "\n",
    "## How Does It Work?\n",
    "\n",
    "1. **Model:**  \n",
    "   The model predicts output as:\n",
    "   $$\n",
    "   h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "   $$\n",
    "\n",
    "2. **Cost Function (Mean Squared Error):**  \n",
    "   Measures how well the line fits the data:\n",
    "   $$\n",
    "   J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "   $$\n",
    "   - \\( m \\): Number of data points  \n",
    "   - \\( h_\\theta(x^{(i)}) \\): Predicted value  \n",
    "   - \\( y^{(i)} \\): Actual value\n",
    "\n",
    "3. **Gradient Descent:**  \n",
    "   An optimization algorithm to minimize the cost function by updating the parameters:\n",
    "   $$\n",
    "   \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)\n",
    "   $$\n",
    "   - \\( \\alpha \\): Learning rate\n",
    "\n",
    "   Update rules:\n",
    "   $$\n",
    "   \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)\n",
    "   $$\n",
    "   $$\n",
    "   \\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}\n",
    "   $$\n",
    "\n",
    "4. **Repeat until convergence:**  \n",
    "   Keep updating $$ \\theta_0 \\ $$ and $$ \\theta_1 \\ $$ until the cost function is minimized (i.e., the line fits the data as best as possible).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- The goal is to find the best-fit line that minimizes the error between predicted and actual values.\n",
    "- The error is minimized using gradient descent.\n",
    "- The cost function used is Mean Squared Error (MSE).\n",
    "- The learning rate (\\( \\alpha \\)) controls how big a step we take in each iteration. Generally taken to be a low value, like 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c433f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
